{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7990e20",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.008184,
     "end_time": "2023-08-05T08:00:30.345756",
     "exception": false,
     "start_time": "2023-08-05T08:00:30.337572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Magical Synergy: The Unbreakable Bond between Data Engineer and ETL\n",
    "\n",
    "As a Data Engineer, your job is to be a wizard who manages and organizes the flow of information within a company or organization. Imagine you have a big box full of different kinds of toys scattered all over the place. ETL is like a magic process that takes all those toys, organizes them neatly into different boxes based on their types, and puts labels on each box so you can easily find what you need later. You as the magician has that magical ability at your disposal. \n",
    "\n",
    "Hereis how the magic works:\n",
    "\n",
    "1. **Extract**: You have the ability to gather data from different places like databases, spreadsheets, or even websites. It's like you can magically reach out and collect all the scattered pieces of information from different sources.\n",
    "\n",
    "2. **Transform**: Now comes the exciting part! You use your magic skills to clean up and transform the data into a more useful and understandable format. It's like turning a bunch of jumbled letters into a clear and meaningful sentence. You can think of it as a clean up and make the future progress easier. \n",
    "\n",
    "3. **Load**: Once the data is all tidy and neat, you store it in a special place, like a data warehouse. It's like putting everything into labeled boxes, so anyone in the company can find what they need easily.\n",
    "\n",
    "By doing all this, you make sure that everyone in the organization has access to accurate and well-organized data. This helps people make better decisions, understand trends, and find answers to important questions. You're like the hero behind the scenes, making sure the company's data is always ready to work its magic and help everyone succeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415c8f3",
   "metadata": {
    "papermill": {
     "duration": 0.007174,
     "end_time": "2023-08-05T08:00:30.360734",
     "exception": false,
     "start_time": "2023-08-05T08:00:30.353560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Installing require libary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5804d8ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:00:30.378990Z",
     "iopub.status.busy": "2023-08-05T08:00:30.377960Z",
     "iopub.status.idle": "2023-08-05T08:02:02.633205Z",
     "shell.execute_reply": "2023-08-05T08:02:02.631379Z"
    },
    "papermill": {
     "duration": 92.268157,
     "end_time": "2023-08-05T08:02:02.636474",
     "exception": false,
     "start_time": "2023-08-05T08:00:30.368317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark -q\n",
    "!pip install yfinance -q\n",
    "!pip install yahoo_fin -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25381f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:02.654629Z",
     "iopub.status.busy": "2023-08-05T08:02:02.654186Z",
     "iopub.status.idle": "2023-08-05T08:02:09.374047Z",
     "shell.execute_reply": "2023-08-05T08:02:09.372594Z"
    },
    "papermill": {
     "duration": 6.732424,
     "end_time": "2023-08-05T08:02:09.376872",
     "exception": false,
     "start_time": "2023-08-05T08:02:02.644448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/05 08:02:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"finance\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51380b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:09.395619Z",
     "iopub.status.busy": "2023-08-05T08:02:09.394423Z",
     "iopub.status.idle": "2023-08-05T08:02:09.406886Z",
     "shell.execute_reply": "2023-08-05T08:02:09.405929Z"
    },
    "papermill": {
     "duration": 0.024257,
     "end_time": "2023-08-05T08:02:09.409278",
     "exception": false,
     "start_time": "2023-08-05T08:02:09.385021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "\n",
    "# Color spectrum\n",
    "COLOR_RED = '\\033[91m'\n",
    "COLOR_GREEN = '\\033[92m'\n",
    "COLOR_BLUE = '\\033[94m'\n",
    "COLOR_YELLOW = '\\033[93m'\n",
    "COLOR_MAGENTA = '\\033[95m'\n",
    "COLOR_CYAN = '\\033[96m'\n",
    "# Text styles\n",
    "TEXT_BOLD = '\\033[1m'\n",
    "TEXT_UNDERLINE = '\\033[4m'\n",
    "# Reset all styles\n",
    "COLOR_RESET = '\\033[0m'\n",
    "\n",
    "def time_func(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        print(f'Function {COLOR_MAGENTA + TEXT_BOLD}{func.__name__}{COLOR_RESET} elapsed time: {COLOR_CYAN + TEXT_BOLD}{time_taken * 1000:.3f}ms{COLOR_RESET}')\n",
    "        print()\n",
    "        return result \n",
    "    return wrapper\n",
    "        \n",
    "def log_function_call(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        arg_str = ', '.join([repr(arg) for arg in args] + [f\"{k}={v!r}\" for k, v in kwargs.items()])\n",
    "        print(f\"Calling {func.__name__}({arg_str})\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"{func.__name__} returned {result!r}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Example functions that use two different wrapper function styles\n",
    "@time_func\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "@log_function_call\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# # Calling the functions\n",
    "# add_result = add(5, 7)\n",
    "# greet_message = greet(\"Alice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790c496",
   "metadata": {
    "papermill": {
     "duration": 0.007787,
     "end_time": "2023-08-05T08:02:09.424892",
     "exception": false,
     "start_time": "2023-08-05T08:02:09.417105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "IMPORTANT DISCLAIMER: Normally ```load()``` function would stored to data we have transformed into a dataware house for later use. However, since I don't have a database or dataware house to store my data, I will slightly change the loading process to storing the data in a format of a PySpark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d49761a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:09.442891Z",
     "iopub.status.busy": "2023-08-05T08:02:09.442099Z",
     "iopub.status.idle": "2023-08-05T08:02:09.522364Z",
     "shell.execute_reply": "2023-08-05T08:02:09.520902Z"
    },
    "papermill": {
     "duration": 0.092508,
     "end_time": "2023-08-05T08:02:09.525166",
     "exception": false,
     "start_time": "2023-08-05T08:02:09.432658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SchemaLibrary():\n",
    "    def __init__(self):\n",
    "        self.financial_schema = T.StructType([\n",
    "            T.StructField(\"ticker\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"date\", T.DateType(), nullable=False),\n",
    "            T.StructField(\"macd\", T.DoubleType(), nullable=False),\n",
    "            T.StructField(\"daily_rtn\", T.DoubleType(), nullable=False),\n",
    "            T.StructField(\"mavg_20_days\", T.DoubleType(), nullable=False),\n",
    "            T.StructField(\"vol_chg\", T.DoubleType(), nullable=False),\n",
    "            T.StructField(\"vol_shock\", T.DoubleType(), nullable=False),\n",
    "            T.StructField(\"sma\", T.DoubleType(), nullable=False),\n",
    "            T.StructField(\"rsi\", T.DoubleType(), nullable=False),\n",
    "            T.StructField(\"created_at\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"dt\", T.StringType(), nullable=False)\n",
    "        ])\n",
    "        \n",
    "        self.log_schema = T.StructType([\n",
    "            T.StructField(\"ticker\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"dt\", T.StringType(), nullable=True),\n",
    "        ])\n",
    "\n",
    "\n",
    "class FinanceUtility():\n",
    "    def __init__(self):\n",
    "        self.window_constraint = Window.partitionBy(F.col('ticker')).orderBy(F.asc(F.col('Date')))\n",
    "    \n",
    "    # Calculate EMA \n",
    "    def calculate_ema(\n",
    "        self,\n",
    "        prices, \n",
    "        days, \n",
    "        smoothing=2.0\n",
    "    ):\n",
    "        # Set the first EMA value equal to the first close price\n",
    "        ema = [prices[0]]  \n",
    "        for price in prices[1:]:\n",
    "            ema.append((price * (smoothing / (1 + days))) + ema[-1] * (1 - (smoothing / (1 + days))))\n",
    "        return ema\n",
    "    \n",
    "    # Calculate MACD =12-Period EMA − 26-Period EMA\n",
    "    def calculate_macd(\n",
    "        self,\n",
    "        prices, \n",
    "        short_period=12, \n",
    "        long_period=26, \n",
    "        smoothing=2.0\n",
    "    ):\n",
    "        ema_short_period = self.calculate_ema(prices, short_period, smoothing)\n",
    "        ema_long_period = self.calculate_ema(prices, long_period, smoothing)\n",
    "        return [x-y for x,y in zip(ema_short_period, ema_long_period)]\n",
    "    \n",
    "    def calculate_daily_return(\n",
    "        self,\n",
    "        sdf\n",
    "    ):\n",
    "        data_sdf = sdf.withColumn('prev_close', F.lag(F.col('close'),1).over(self.window_constraint))\n",
    "        data_sdf = data_sdf.withColumn('daily_rtn', (F.col('close') - F.col('prev_close')) / F.col('prev_close'))  \n",
    "\n",
    "        data_sdf = data_sdf.drop(*['prev_close'])\n",
    "        return data_sdf\n",
    "\n",
    "    def calculate_moving_average(\n",
    "        self,\n",
    "        sdf, \n",
    "        window=20\n",
    "    ):\n",
    "        moving_avg_name = f\"mavg_{window}_days\"\n",
    "        data_sdf = sdf.withColumn(moving_avg_name, F.avg(F.col('close')).over(self.window_constraint.rowsBetween(-(window-1),0)))\n",
    "        return data_sdf\n",
    "\n",
    "    def calculate_volume_change(\n",
    "        self,\n",
    "        sdf\n",
    "    ):\n",
    "        data_sdf = sdf.withColumn('prev_volume', F.lag(F.col('volume'),1).over(self.window_constraint))\n",
    "        data_sdf = data_sdf.withColumn('vol_chg', (F.col('Volume') - F.col('prev_volume')) / F.col('prev_volume'))\n",
    "\n",
    "        data_sdf = data_sdf.drop(*['prev_volume'])\n",
    "        return data_sdf\n",
    "\n",
    "    def calculate_volume_shock(\n",
    "        self,\n",
    "        sdf, \n",
    "        volume_threshold=10\n",
    "    ):\n",
    "        data_sdf = sdf.withColumn('vol_shock', F.when(F.col('vol_chg') > volume_threshold, 1).otherwise(0))\n",
    "        return data_sdf\n",
    "\n",
    "    def calculate_sma(\n",
    "        self,\n",
    "        sdf, \n",
    "        window=20\n",
    "    ):\n",
    "        window_constraint = self.window_constraint.rowsBetween(-window+1,0)\n",
    "        return sdf.withColumn('sma', F.avg(F.col('close')).over(window_constraint))\n",
    "\n",
    "    def calculate_rsi(\n",
    "        self,\n",
    "        sdf,\n",
    "        window =14\n",
    "    ):\n",
    "        # Daily_Price_Change = Close(t) - Close(t-1)\n",
    "        data_sdf = sdf.withColumn('daily_price_change', F.col('close') - F.lag(F.col('close'),1).over(self.window_constraint))\n",
    "\n",
    "        # Gain(t) = max(0, Daily_Price_Change), Loss(t) = max(0, -Daily_Price_Change)\n",
    "        data_sdf = data_sdf.withColumn(\n",
    "            'gain', \n",
    "            F.when(F.col('daily_price_change') > 0, F.col('daily_price_change')).otherwise(0)\n",
    "        )\n",
    "        data_sdf = data_sdf.withColumn(\n",
    "            \"loss\", \n",
    "            F.when(F.col(\"daily_price_change\") < 0, F.abs(F.col(\"daily_price_change\"))).otherwise(0)\n",
    "        )\n",
    "\n",
    "        # AG = (sum of Gain(t) for the last 'n' days) / 'n', AL = (sum of Loss(t) for the last 'n' days) / 'n'\n",
    "        window_rsi_constraint = self.window_constraint.rowsBetween(-window+1,0)\n",
    "        data_sdf = data_sdf.withColumn('avg_gain', F.avg(\"gain\").over(window_rsi_constraint))\n",
    "        data_sdf = data_sdf.withColumn('avg_loss', F.avg(\"loss\").over(window_rsi_constraint))\n",
    "\n",
    "        # RS = AG / AL\n",
    "        data_sdf = data_sdf.withColumn('rs', F.col('avg_gain') / F.col('avg_loss')).fillna(0.0)\n",
    "\n",
    "        # RSI = 100 - (100 / (1 + RS))\n",
    "        data_sdf =  data_sdf.withColumn('rsi', 100 - (100 / (1 + F.col('rs'))))\n",
    "\n",
    "        data_sdf = data_sdf.drop(*['daily_price_change','gain','loss','avg_gain','avg_loss','rs'])\n",
    "\n",
    "        return data_sdf\n",
    "\n",
    "\n",
    "class FinanceETL():\n",
    "    def __init__(\n",
    "        self, \n",
    "        ticker, \n",
    "        start_date, \n",
    "        end_date\n",
    "    ):\n",
    "        self.ticker = ticker\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.utility = FinanceUtility()\n",
    "        self.schema = SchemaLibrary()\n",
    "        \n",
    "    \n",
    "    def extract(\n",
    "        self\n",
    "    ):\n",
    "        '''\n",
    "        Extract daily stock data from start to end date into pandas dataframe\n",
    "        Data is downloaded from Yahoo Finance\n",
    "        Return df and flag (1: empty dataframe, 0: otherwise)\n",
    "        '''\n",
    "        df = yf.download(\n",
    "            self.ticker,\n",
    "            start = self.start_date,\n",
    "            end = self.end_date,\n",
    "            interval = '1d',\n",
    "            threads = True\n",
    "        ).reset_index()\n",
    "        \n",
    "        return df, 1 if len(df) == 0 else 0\n",
    "    \n",
    "    def transform(\n",
    "        self, \n",
    "        df\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformation using Pandas\n",
    "        1. Add Stock ticker (ex. AAPL, MSFT, etc.)\n",
    "        2. Calculate MACD based on Close Price for each entry\n",
    "        \n",
    "        Transformation using PySpark\n",
    "        1. Calculate Daily Return\n",
    "        2. Calculate 20 Days Moving Average\n",
    "        3. Calculate Volume Change\n",
    "        4. Calculate Volume Shock based on Volume Change\n",
    "        5. Calculate Simple Moving Average (SMA)\n",
    "        6. Calculate RSI\n",
    "        7. Add created_dt and dt for partition purposes\n",
    "        \"\"\"\n",
    "        # Pandas Dataframe Transformation\n",
    "        df['ticker'] = self.ticker\n",
    "        df['macd'] = self.utility.calculate_macd(df['Close'])\n",
    "        \n",
    "        # Spark Dataframe Transformation\n",
    "        sdf = spark.createDataFrame(df).select(['ticker','Date','Close','Volume','macd'])\n",
    "        sdf = sdf.withColumnRenamed('Close','close').withColumnRenamed('Volume','volume')\n",
    "        sdf = sdf.withColumnRenamed('Date','date').withColumn('date', F.to_date(F.col('date')))\n",
    "        \n",
    "        sdf = self.utility.calculate_daily_return(sdf)\n",
    "        sdf = self.utility.calculate_moving_average(sdf)\n",
    "        sdf = self.utility.calculate_volume_change(sdf)\n",
    "        sdf = self.utility.calculate_volume_shock(sdf)\n",
    "        sdf = self.utility.calculate_sma(sdf)\n",
    "        sdf = self.utility.calculate_rsi(sdf)\n",
    "        \n",
    "        sdf = sdf.drop(*['close','volume'])\n",
    "        \n",
    "        sdf = sdf.withColumn('created_dt', F.lit(datetime.now()))\n",
    "        sdf = sdf.withColumn('dt', F.lit(datetime.now().strftime(\"%Y-%m-%d\")))\n",
    "        \n",
    "        return sdf\n",
    "    \n",
    "    def load(\n",
    "        self,\n",
    "        data_sdf,\n",
    "        sdf\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update Stock information\n",
    "        Add Partition to make sure that we update based on the most recent data (latest)\n",
    "        \"\"\"\n",
    "        # Add another condition to check whether the ticker is already inside the pyspark dataframe\n",
    "        # If it already is, then we have to use the Window function; otherwise, we can union them\n",
    "        # However, if this process still takes too much memory time, then I have to revise the loading OR the overall code structure\n",
    "        window_latest_info = Window.partitionBy(F.col('ticker'), F.col('date'), F.col('dt')).orderBy(F.desc(F.col('created_at')))\n",
    "        updated_sdf = (\n",
    "            data_sdf.unionAll(sdf)\n",
    "            .withColumn('latest_ranking', F.row_number().over(window_latest_info))\n",
    "            .filter(F.col('latest_ranking') == 1)\n",
    "        ).drop('latest_ranking')\n",
    "        \n",
    "        return updated_sdf\n",
    "    \n",
    "    def etl_log_load(\n",
    "        self,\n",
    "        log_sdf,\n",
    "        ticker,\n",
    "        dt\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a dataframe that log all the etl command execution for both successful and unsucessful attempt\n",
    "        \"\"\"\n",
    "        sdf = spark.createDataFrame([(ticker,dt)], self.schema.log_schema)\n",
    "        return log_sdf.union(sdf)\n",
    "    \n",
    "    @time_func\n",
    "    def etl_execution(\n",
    "        self,\n",
    "        data_sdf,\n",
    "        log_sdf\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Execute the function Extract, Transform, and \n",
    "        \"\"\"\n",
    "        print(f'{COLOR_BLUE}Process: etl_execution has started at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} for {self.ticker}{COLOR_RESET}')\n",
    "        df, is_empty = self.extract()\n",
    "        \n",
    "        if is_empty == 1:\n",
    "            error_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f'{COLOR_RED}ERROR: etl process for {self.ticker} has failed at {error_datetime}{COLOR_RESET}')\n",
    "            return data_sdf, self.etl_log_load(log_sdf, self.ticker, error_datetime)\n",
    "        else:\n",
    "            sdf = self.transform(df)\n",
    "            print(f'{COLOR_GREEN}COMPLETE: etl process for {self.ticker}{COLOR_RESET}') \n",
    "            return self.load(data_sdf, sdf), self.etl_log_load(log_sdf, self.ticker, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c0ea0",
   "metadata": {
    "papermill": {
     "duration": 0.0073,
     "end_time": "2023-08-05T08:02:09.540242",
     "exception": false,
     "start_time": "2023-08-05T08:02:09.532942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Example ETL Execution\n",
    "\n",
    "Now, we will be testing the code. I have selected two stocks (AAPl and TSLA) with one additional imaginary stock called (DUMMY). \"DUMMY\" will be used as a test case where such stock ticker doesn't exist. The process must be able to capture this error, store to the information, and report the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a72bb17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:09.559565Z",
     "iopub.status.busy": "2023-08-05T08:02:09.558194Z",
     "iopub.status.idle": "2023-08-05T08:02:27.336867Z",
     "shell.execute_reply": "2023-08-05T08:02:27.335654Z"
    },
    "papermill": {
     "duration": 17.792941,
     "end_time": "2023-08-05T08:02:27.342187",
     "exception": false,
     "start_time": "2023-08-05T08:02:09.549246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:13 for AAPL\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for AAPL\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m2199.700ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:16 for TSLA\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for TSLA\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m1086.315ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:17 for DUMMY\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[91mERROR: etl process for DUMMY has failed at 2023-08-05 08:02:17\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m260.550ms\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|ticker|count|\n",
      "+------+-----+\n",
      "|  AAPL|  139|\n",
      "|  TSLA|  139|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==============================================>          (9 + 2) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|ticker|                 dt|\n",
      "+------+-------------------+\n",
      "| DUMMY|2023-08-05 08:02:17|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Setup Schema and Empty Spark Dataframe for storing information\n",
    "schema = SchemaLibrary()\n",
    "financial_sdf = spark.createDataFrame([], schema.financial_schema)\n",
    "log_sdf = spark.createDataFrame([], schema.log_schema)\n",
    "\n",
    "# Setup start and end date for retriving the information\n",
    "current_time = datetime.now()\n",
    "start_date = current_time- timedelta(days = 200) # Prior two years\n",
    "end_date = current_time.strftime('%Y-%m-%d')\n",
    "\n",
    "# Example Execution of ETL\n",
    "for ticker in ['AAPL','TSLA','DUMMY']:\n",
    "    etl = FinanceETL(ticker=ticker, start_date=start_date, end_date=end_date)\n",
    "    financial_sdf, log_sdf = etl.etl_execution(financial_sdf, log_sdf)\n",
    "\n",
    "# Display the result\n",
    "financial_sdf.groupBy(F.col('ticker')).count().show()\n",
    "log_sdf.filter(F.col('dt').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41975e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:27.374648Z",
     "iopub.status.busy": "2023-08-05T08:02:27.374113Z",
     "iopub.status.idle": "2023-08-05T08:02:30.999512Z",
     "shell.execute_reply": "2023-08-05T08:02:30.998372Z"
    },
    "papermill": {
     "duration": 3.645986,
     "end_time": "2023-08-05T08:02:31.002002",
     "exception": false,
     "start_time": "2023-08-05T08:02:27.356016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>macd</th>\n",
       "      <th>daily_rtn</th>\n",
       "      <th>mavg_20_days</th>\n",
       "      <th>vol_chg</th>\n",
       "      <th>vol_shock</th>\n",
       "      <th>sma</th>\n",
       "      <th>rsi</th>\n",
       "      <th>created_at</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.888196</td>\n",
       "      <td>-0.048020</td>\n",
       "      <td>192.165001</td>\n",
       "      <td>0.891064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.165001</td>\n",
       "      <td>27.203654</td>\n",
       "      <td>2023-08-05 08:02:15.567891</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.475714</td>\n",
       "      <td>-0.021055</td>\n",
       "      <td>269.260000</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.260000</td>\n",
       "      <td>30.652676</td>\n",
       "      <td>2023-08-05 08:02:16.922907</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      macd  daily_rtn  mavg_20_days   vol_chg  vol_shock  \\\n",
       "0   AAPL  2023-08-04  0.888196  -0.048020    192.165001  0.891064        0.0   \n",
       "1   TSLA  2023-08-04  0.475714  -0.021055    269.260000  0.017152        0.0   \n",
       "\n",
       "          sma        rsi                  created_at          dt  \n",
       "0  192.165001  27.203654  2023-08-05 08:02:15.567891  2023-08-05  \n",
       "1  269.260000  30.652676  2023-08-05 08:02:16.922907  2023-08-05  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For display purposes\n",
    "(\n",
    "    financial_sdf\n",
    "    .withColumn('disp_rank', F.row_number().over(Window.partitionBy(F.col('ticker')).orderBy(F.desc(F.col('date')))))\n",
    "    .filter(F.col('disp_rank').isin([1]))\n",
    "    .drop(*['disp_rank'])\n",
    ").toPandas().head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5415552",
   "metadata": {
    "papermill": {
     "duration": 0.00987,
     "end_time": "2023-08-05T08:02:31.021953",
     "exception": false,
     "start_time": "2023-08-05T08:02:31.012083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Testing another execution\n",
    "Given the result from the previous ETL pipeline, I would like to see whether I can use the same procedure to add new stock or replace the exisiting stock with more recent data or not. Hence, I have selected two testing stocks as AAPL and MSFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6555a816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:31.044169Z",
     "iopub.status.busy": "2023-08-05T08:02:31.043734Z",
     "iopub.status.idle": "2023-08-05T08:02:39.664943Z",
     "shell.execute_reply": "2023-08-05T08:02:39.663765Z"
    },
    "papermill": {
     "duration": 8.636102,
     "end_time": "2023-08-05T08:02:39.668350",
     "exception": false,
     "start_time": "2023-08-05T08:02:31.032248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:31 for MSFT\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for MSFT\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m927.214ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:32 for AAPL\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for AAPL\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m852.233ms\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|ticker|count|\n",
      "+------+-----+\n",
      "|  AAPL|  139|\n",
      "|  TSLA|  139|\n",
      "|  MSFT|  139|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:===============================================>        (16 + 3) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|ticker|                 dt|\n",
      "+------+-------------------+\n",
      "| DUMMY|2023-08-05 08:02:17|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Repeat the ETL process specifically for MSFT and AAPL (again)\n",
    "msft_etl = FinanceETL(ticker='MSFT', start_date=start_date, end_date=end_date)\n",
    "financial_sdf, log_sdf = msft_etl.etl_execution(financial_sdf, log_sdf)\n",
    "\n",
    "aapl_etl = FinanceETL(ticker='AAPL', start_date=start_date, end_date=end_date)\n",
    "financial_sdf, log_sdf = aapl_etl.etl_execution(financial_sdf, log_sdf)\n",
    "\n",
    "# Display the result\n",
    "financial_sdf.groupBy(F.col('ticker')).count().show()\n",
    "log_sdf.filter(F.col('dt').isNotNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6400ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:39.699519Z",
     "iopub.status.busy": "2023-08-05T08:02:39.698870Z",
     "iopub.status.idle": "2023-08-05T08:02:44.422010Z",
     "shell.execute_reply": "2023-08-05T08:02:44.420906Z"
    },
    "papermill": {
     "duration": 4.740478,
     "end_time": "2023-08-05T08:02:44.425947",
     "exception": false,
     "start_time": "2023-08-05T08:02:39.685469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>macd</th>\n",
       "      <th>daily_rtn</th>\n",
       "      <th>mavg_20_days</th>\n",
       "      <th>vol_chg</th>\n",
       "      <th>vol_shock</th>\n",
       "      <th>sma</th>\n",
       "      <th>rsi</th>\n",
       "      <th>created_at</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.888196</td>\n",
       "      <td>-0.048020</td>\n",
       "      <td>192.165001</td>\n",
       "      <td>0.891064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.165001</td>\n",
       "      <td>27.203654</td>\n",
       "      <td>2023-08-05 08:02:32.623885</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>-1.654641</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>339.874498</td>\n",
       "      <td>0.299884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>339.874498</td>\n",
       "      <td>38.533268</td>\n",
       "      <td>2023-08-05 08:02:31.822584</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.475714</td>\n",
       "      <td>-0.021055</td>\n",
       "      <td>269.260000</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.260000</td>\n",
       "      <td>30.652676</td>\n",
       "      <td>2023-08-05 08:02:16.922907</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      macd  daily_rtn  mavg_20_days   vol_chg  vol_shock  \\\n",
       "0   AAPL  2023-08-04  0.888196  -0.048020    192.165001  0.891064        0.0   \n",
       "1   MSFT  2023-08-04 -1.654641   0.003429    339.874498  0.299884        0.0   \n",
       "2   TSLA  2023-08-04  0.475714  -0.021055    269.260000  0.017152        0.0   \n",
       "\n",
       "          sma        rsi                  created_at          dt  \n",
       "0  192.165001  27.203654  2023-08-05 08:02:32.623885  2023-08-05  \n",
       "1  339.874498  38.533268  2023-08-05 08:02:31.822584  2023-08-05  \n",
       "2  269.260000  30.652676  2023-08-05 08:02:16.922907  2023-08-05  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For display purposes for testing another execution\n",
    "(\n",
    "    financial_sdf\n",
    "    .withColumn('disp_rank', F.row_number().over(Window.partitionBy(F.col('ticker')).orderBy(F.desc(F.col('date')))))\n",
    "    .filter(F.col('disp_rank').isin([1]))\n",
    "    .drop(*['disp_rank'])\n",
    ").toPandas().head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf8675",
   "metadata": {
    "papermill": {
     "duration": 0.017807,
     "end_time": "2023-08-05T08:02:44.461860",
     "exception": false,
     "start_time": "2023-08-05T08:02:44.444053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ETL Repiar kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39042cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:44.501127Z",
     "iopub.status.busy": "2023-08-05T08:02:44.500117Z",
     "iopub.status.idle": "2023-08-05T08:02:44.516219Z",
     "shell.execute_reply": "2023-08-05T08:02:44.515038Z"
    },
    "papermill": {
     "duration": 0.039306,
     "end_time": "2023-08-05T08:02:44.519397",
     "exception": false,
     "start_time": "2023-08-05T08:02:44.480091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create ETL repair\n",
    "class FinanceETLRepair():\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        log\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        \n",
    "    def extract_error_log(self):\n",
    "        \"\"\"\n",
    "        Return PySpark dataframe of error ticker and download time\n",
    "        \"\"\"\n",
    "        return self.log.filter(F.col('dt').isNotNull())\n",
    "    \n",
    "    def extract_ticker_list(self):\n",
    "        \"\"\"\n",
    "        Return List of Distinct Stock ticker that failed to download from previous ETL process\n",
    "        \"\"\"\n",
    "        log_sdf = self.extract_error_log()\n",
    "        return log_sdf.select(['ticker']).distinct().toPandas()['ticker'].tolist()\n",
    "    \n",
    "    def single_repair_derror(\n",
    "        self, \n",
    "        ticker,\n",
    "        start_date, \n",
    "        end_date\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Insert new data for single stock ticker\n",
    "        \"\"\"\n",
    "        etl_finance = FinanceETL(ticker, start_date=start_date, end_date=end_date)\n",
    "        self.data, self.log = etl_finance.etl_execution(self.data, self.log)\n",
    "        return self.data, self.log\n",
    "        \n",
    "    def multiple_repair_derror(\n",
    "        self, \n",
    "        ticker_list, \n",
    "        start_date, \n",
    "        end_date\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Insert new data for multiple stock tickers (input in the format of a list)\n",
    "        \"\"\"\n",
    "        for ticker in ticker_list:\n",
    "            etl_finance = FinanceETL(ticker, start_date=start_date, end_date=end_date)\n",
    "            self.data, self.log = etl_finance.etl_execution(self.data, self.log)\n",
    "        return self.data, self.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b0b08",
   "metadata": {
    "papermill": {
     "duration": 0.017705,
     "end_time": "2023-08-05T08:02:44.554724",
     "exception": false,
     "start_time": "2023-08-05T08:02:44.537019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Disclamier: testing the ETL repair kit\n",
    "\n",
    "For testing purposes, I will modified the stock inside the error_ticker_list to be something completely new like ```['CRM','UBER','IBM']```. Also, I will demonstrate that ```single_repair_derror()``` function will overwrite the existing data incase of single group of data is needed etl exeuction again. I will be using MSFT as testing stock. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f78b7ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:44.593258Z",
     "iopub.status.busy": "2023-08-05T08:02:44.592284Z",
     "iopub.status.idle": "2023-08-05T08:02:56.929349Z",
     "shell.execute_reply": "2023-08-05T08:02:56.928086Z"
    },
    "papermill": {
     "duration": 12.360565,
     "end_time": "2023-08-05T08:02:56.933108",
     "exception": false,
     "start_time": "2023-08-05T08:02:44.572543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:47 for MSFT\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for MSFT\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m747.654ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:48 for CRM\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for CRM\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m1068.787ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:49 for UBER\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for UBER\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m973.877ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:02:50 for IBM\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for IBM\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m889.390ms\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|ticker|count|\n",
      "+------+-----+\n",
      "|  AAPL|  139|\n",
      "|  TSLA|  139|\n",
      "|   IBM|  139|\n",
      "|  UBER|  139|\n",
      "|  MSFT|  139|\n",
      "|   CRM|  139|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Execute the Repair process\n",
    "etl_repair = FinanceETLRepair(financial_sdf, log_sdf)\n",
    "error_log = etl_repair.extract_error_log()\n",
    "error_ticker_list = etl_repair.extract_ticker_list() # [\"DUMMY\"]\n",
    "error_ticker_list = ['CRM','UBER','IBM']\n",
    "\n",
    "financial_sdf, log_sdf = etl_repair.single_repair_derror('MSFT', start_date, end_date)\n",
    "financial_sdf, log_sdf= etl_repair.multiple_repair_derror(error_ticker_list, start_date, end_date)\n",
    "\n",
    "financial_sdf.groupBy(F.col('ticker')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e4823a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:02:56.971939Z",
     "iopub.status.busy": "2023-08-05T08:02:56.971496Z",
     "iopub.status.idle": "2023-08-05T08:03:04.612888Z",
     "shell.execute_reply": "2023-08-05T08:03:04.611664Z"
    },
    "papermill": {
     "duration": 7.661306,
     "end_time": "2023-08-05T08:03:04.616405",
     "exception": false,
     "start_time": "2023-08-05T08:02:56.955099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>macd</th>\n",
       "      <th>daily_rtn</th>\n",
       "      <th>mavg_20_days</th>\n",
       "      <th>vol_chg</th>\n",
       "      <th>vol_shock</th>\n",
       "      <th>sma</th>\n",
       "      <th>rsi</th>\n",
       "      <th>created_at</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.888196</td>\n",
       "      <td>-0.048020</td>\n",
       "      <td>192.165001</td>\n",
       "      <td>0.891064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.165001</td>\n",
       "      <td>27.203654</td>\n",
       "      <td>2023-08-05 08:02:32.623885</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRM</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.879956</td>\n",
       "      <td>-0.004500</td>\n",
       "      <td>224.756499</td>\n",
       "      <td>-0.129714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>224.756499</td>\n",
       "      <td>26.498415</td>\n",
       "      <td>2023-08-05 08:02:48.857252</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>3.006501</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>138.881499</td>\n",
       "      <td>0.068625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.881499</td>\n",
       "      <td>91.254246</td>\n",
       "      <td>2023-08-05 08:02:50.759943</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>-1.654641</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>339.874498</td>\n",
       "      <td>0.299884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>339.874498</td>\n",
       "      <td>38.533268</td>\n",
       "      <td>2023-08-05 08:02:47.842017</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.475714</td>\n",
       "      <td>-0.021055</td>\n",
       "      <td>269.260000</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.260000</td>\n",
       "      <td>30.652676</td>\n",
       "      <td>2023-08-05 08:02:16.922907</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UBER</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.974281</td>\n",
       "      <td>-0.015465</td>\n",
       "      <td>46.331000</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.331000</td>\n",
       "      <td>48.730559</td>\n",
       "      <td>2023-08-05 08:02:49.781731</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      macd  daily_rtn  mavg_20_days   vol_chg  vol_shock  \\\n",
       "0   AAPL  2023-08-04  0.888196  -0.048020    192.165001  0.891064        0.0   \n",
       "1    CRM  2023-08-04  0.879956  -0.004500    224.756499 -0.129714        0.0   \n",
       "2    IBM  2023-08-04  3.006501  -0.001454    138.881499  0.068625        0.0   \n",
       "3   MSFT  2023-08-04 -1.654641   0.003429    339.874498  0.299884        0.0   \n",
       "4   TSLA  2023-08-04  0.475714  -0.021055    269.260000  0.017152        0.0   \n",
       "5   UBER  2023-08-04  0.974281  -0.015465     46.331000  0.015478        0.0   \n",
       "\n",
       "          sma        rsi                  created_at          dt  \n",
       "0  192.165001  27.203654  2023-08-05 08:02:32.623885  2023-08-05  \n",
       "1  224.756499  26.498415  2023-08-05 08:02:48.857252  2023-08-05  \n",
       "2  138.881499  91.254246  2023-08-05 08:02:50.759943  2023-08-05  \n",
       "3  339.874498  38.533268  2023-08-05 08:02:47.842017  2023-08-05  \n",
       "4  269.260000  30.652676  2023-08-05 08:02:16.922907  2023-08-05  \n",
       "5   46.331000  48.730559  2023-08-05 08:02:49.781731  2023-08-05  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For display purposes for testing ETL repair kit\n",
    "(\n",
    "    financial_sdf\n",
    "    .withColumn('disp_rank', F.row_number().over(Window.partitionBy(F.col('ticker')).orderBy(F.desc(F.col('date')))))\n",
    "    .filter(F.col('disp_rank').isin([1]))\n",
    "    .drop(*['disp_rank'])\n",
    ").toPandas().head(n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee133c9",
   "metadata": {
    "papermill": {
     "duration": 0.022305,
     "end_time": "2023-08-05T08:03:04.662448",
     "exception": false,
     "start_time": "2023-08-05T08:03:04.640143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Running the ETL Pipeline on larger set of stocks\n",
    "There is a lot of data to be stored in the Dataframe. I have split the list of stocks into smaller partition can create seperate dataframe for storing the informatio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c97164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:03:04.697216Z",
     "iopub.status.busy": "2023-08-05T08:03:04.696760Z",
     "iopub.status.idle": "2023-08-05T08:03:41.961442Z",
     "shell.execute_reply": "2023-08-05T08:03:41.960491Z"
    },
    "papermill": {
     "duration": 37.285537,
     "end_time": "2023-08-05T08:03:41.964159",
     "exception": false,
     "start_time": "2023-08-05T08:03:04.678622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION 1 BEGINS: ------>\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:05 for ARE\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for ARE\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m791.528ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:06 for AEP\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for AEP\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m871.886ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:07 for AME\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for AME\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m824.006ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:08 for MO\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for MO\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m810.259ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:09 for AES\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for AES\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m704.181ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:09 for ADM\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for ADM\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m736.319ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:10 for ALL\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for ALL\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m722.788ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:11 for CPT\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for CPT\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m871.236ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:12 for T\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for T\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m751.702ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:12 for AIG\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for AIG\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m796.518ms\u001b[0m\n",
      "\n",
      "PARTITION 2 BEGINS: ------>\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:13 for FE\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for FE\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m665.138ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:14 for EXPE\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for EXPE\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m586.917ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:15 for C\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for C\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m701.488ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:15 for DE\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for DE\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m812.284ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:16 for CFG\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for CFG\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m688.485ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:17 for FICO\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for FICO\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m753.502ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:18 for COO\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for COO\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m775.331ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:18 for COST\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for COST\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m736.023ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:19 for CHTR\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for CHTR\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m665.455ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:20 for FAST\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for FAST\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m755.919ms\u001b[0m\n",
      "\n",
      "PARTITION 3 BEGINS: ------>\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:21 for GRMN\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for GRMN\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m621.762ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:21 for JCI\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for JCI\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m664.807ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:22 for KMI\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for KMI\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m686.754ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:23 for HPE\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for HPE\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m624.381ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:23 for BEN\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for BEN\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m689.656ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:24 for J\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for J\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m711.812ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:25 for IR\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for IR\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m660.022ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:25 for HAS\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for HAS\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m756.043ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:26 for LYB\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for LYB\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m682.132ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:27 for HON\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for HON\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m817.024ms\u001b[0m\n",
      "\n",
      "PARTITION 4 BEGINS: ------>\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:28 for MSCI\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for MSCI\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m638.996ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:28 for ODFL\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for ODFL\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m622.702ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:29 for PKG\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for PKG\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m634.340ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:30 for PG\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for PG\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m767.508ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:30 for MOH\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for MOH\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m583.534ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:31 for PANW\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for PANW\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m610.161ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:32 for OMC\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for OMC\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m725.605ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:32 for MTD\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for MTD\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m648.810ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:33 for PXD\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for PXD\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m688.288ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:34 for MCO\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for MCO\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m729.357ms\u001b[0m\n",
      "\n",
      "PARTITION 5 BEGINS: ------>\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:34 for XYL\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for XYL\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m583.043ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:35 for WTW\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for WTW\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m666.537ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:36 for ZION\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for ZION\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m749.708ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:36 for SWK\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for SWK\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m719.530ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:37 for SBAC\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for SBAC\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m679.436ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:38 for STX\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for STX\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m715.183ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:39 for NOW\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for NOW\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m692.124ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:39 for WAT\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for WAT\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m661.278ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:40 for ROP\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for ROP\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m718.298ms\u001b[0m\n",
      "\n",
      "\u001b[94mProcess: etl_execution has started at 2023-08-05 08:03:41 for WEC\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\u001b[92mCOMPLETE: etl process for WEC\u001b[0m\n",
      "Function \u001b[95m\u001b[1metl_execution\u001b[0m elapsed time: \u001b[96m\u001b[1m775.353ms\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from yahoo_fin.stock_info import tickers_nasdaq, tickers_other\n",
    "import random\n",
    "\n",
    "class Stocks():\n",
    "    def extract_nasdaq_tickers(self):\n",
    "        return tickers_nasdaq()\n",
    "    \n",
    "    def extract_nasdaq100_df(self):\n",
    "        df = pd.read_html('https://en.wikipedia.org/wiki/Nasdaq-100')[4]\n",
    "        return df, df['Ticker'].tolist()\n",
    "    \n",
    "    def extract_sp500_df(self):\n",
    "        df = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "        return df, df['Symbol'].to_list()\n",
    "    \n",
    "    def extract_other_tickers(self):\n",
    "        return tickers_other()\n",
    "    \n",
    "    def extract_all_tickers(self):\n",
    "        return self.extract_nasdaq_tickers() + self.extract_other_tickers()\n",
    "\n",
    "def split_into_partitions(arr):\n",
    "    n = len(arr)\n",
    "    partition_size = n // 5  # Size of each partition (5 partitions)\n",
    "    remainder = n % 5  # Remainder elements after forming 5 partitions\n",
    "\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    for i in range(5):\n",
    "        end = start + partition_size\n",
    "        if i < remainder:\n",
    "            end += 1\n",
    "        partitions.append(arr[start:end])\n",
    "        start = end\n",
    "\n",
    "    return partitions\n",
    "\n",
    "# Example usage:\n",
    "sp500_df, sp500_stock_tickers = Stocks().extract_sp500_df()\n",
    "splitted_result = split_into_partitions(sp500_stock_tickers)\n",
    "\n",
    "def database_params_setup():\n",
    "    schema = SchemaLibrary()\n",
    "    sdf = spark.createDataFrame([], schema.financial_schema)\n",
    "    log_sdf = spark.createDataFrame([], schema.log_schema)\n",
    "    \n",
    "    return sdf, log_sdf\n",
    "\n",
    "# Create Database with dictionary as the structure\n",
    "db = {}\n",
    "\n",
    "# Setup start and end date for retriving the information\n",
    "current_time = datetime.now()\n",
    "start_date = current_time - timedelta(days = 200) # Prior two hundred days\n",
    "end_date = current_time.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Execute the ETL for each stock for each partition\n",
    "for index,partition in enumerate(splitted_result):\n",
    "    print(f\"PARTITION {index+1} BEGINS: ------>\")\n",
    "    sdf, log_sdf = database_params_setup()\n",
    "    for ticker in random.sample(partition,10):\n",
    "        etl = FinanceETL(ticker=ticker, start_date=start_date, end_date=end_date)\n",
    "        sdf, log_sdf = etl.etl_execution(sdf, log_sdf)\n",
    "    db[f'stock_{index+1}'] = {}\n",
    "    db[f'stock_{index+1}']['sdf'] = sdf\n",
    "    db[f'stock_{index+1}']['log_sdf'] = log_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c4884a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T08:03:42.026293Z",
     "iopub.status.busy": "2023-08-05T08:03:42.025552Z",
     "iopub.status.idle": "2023-08-05T08:04:29.522648Z",
     "shell.execute_reply": "2023-08-05T08:04:29.521072Z"
    },
    "papermill": {
     "duration": 47.532203,
     "end_time": "2023-08-05T08:04:29.526447",
     "exception": false,
     "start_time": "2023-08-05T08:03:41.994244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 08:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1604.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>macd</th>\n",
       "      <th>daily_rtn</th>\n",
       "      <th>mavg_20_days</th>\n",
       "      <th>vol_chg</th>\n",
       "      <th>vol_shock</th>\n",
       "      <th>sma</th>\n",
       "      <th>rsi</th>\n",
       "      <th>created_at</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADM</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>2.420522</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>82.734000</td>\n",
       "      <td>0.610674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.734000</td>\n",
       "      <td>74.094938</td>\n",
       "      <td>2023-08-05 08:03:10.462351</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEP</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>-0.610782</td>\n",
       "      <td>-0.009147</td>\n",
       "      <td>85.614999</td>\n",
       "      <td>-0.215618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.614999</td>\n",
       "      <td>32.565772</td>\n",
       "      <td>2023-08-05 08:03:07.372058</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AES</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>-0.141604</td>\n",
       "      <td>-0.048910</td>\n",
       "      <td>21.471500</td>\n",
       "      <td>0.276678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.471500</td>\n",
       "      <td>34.895846</td>\n",
       "      <td>2023-08-05 08:03:09.730769</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIG</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>1.018229</td>\n",
       "      <td>-0.007360</td>\n",
       "      <td>59.767500</td>\n",
       "      <td>-0.161527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.767500</td>\n",
       "      <td>63.406400</td>\n",
       "      <td>2023-08-05 08:03:13.581748</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALL</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>0.457975</td>\n",
       "      <td>-0.013380</td>\n",
       "      <td>108.904000</td>\n",
       "      <td>-0.443291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.904000</td>\n",
       "      <td>62.400267</td>\n",
       "      <td>2023-08-05 08:03:11.177286</td>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date      macd  daily_rtn  mavg_20_days   vol_chg  vol_shock  \\\n",
       "0    ADM  2023-08-04  2.420522   0.006826     82.734000  0.610674        0.0   \n",
       "1    AEP  2023-08-04 -0.610782  -0.009147     85.614999 -0.215618        0.0   \n",
       "2    AES  2023-08-04 -0.141604  -0.048910     21.471500  0.276678        0.0   \n",
       "3    AIG  2023-08-04  1.018229  -0.007360     59.767500 -0.161527        0.0   \n",
       "4    ALL  2023-08-04  0.457975  -0.013380    108.904000 -0.443291        0.0   \n",
       "\n",
       "          sma        rsi                  created_at          dt  \n",
       "0   82.734000  74.094938  2023-08-05 08:03:10.462351  2023-08-05  \n",
       "1   85.614999  32.565772  2023-08-05 08:03:07.372058  2023-08-05  \n",
       "2   21.471500  34.895846  2023-08-05 08:03:09.730769  2023-08-05  \n",
       "3   59.767500  63.406400  2023-08-05 08:03:13.581748  2023-08-05  \n",
       "4  108.904000  62.400267  2023-08-05 08:03:11.177286  2023-08-05  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = SchemaLibrary()\n",
    "stock_sdf = spark.createDataFrame([], schema.financial_schema)\n",
    "\n",
    "for key, value in db.items():\n",
    "    stock_sdf = stock_sdf.unionAll(db[key]['sdf'].filter(F.col('date') == (current_time - timedelta(days=1)).strftime('%Y-%m-%d')))\n",
    "\n",
    "stock_sdf.toPandas().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 254.321303,
   "end_time": "2023-08-05T08:04:32.242148",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-05T08:00:17.920845",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
